{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import os\n",
    "import sys\n",
    "# This is done so that the notebook can see files\n",
    "# placed in a different folder\n",
    "module_path = os.path.abspath(os.path.join('..'))\n",
    "if module_path not in sys.path:\n",
    "    sys.path.append(module_path)\n",
    "\n",
    "from environment.Environment import Env2048\n",
    "\n",
    "import reverb\n",
    "\n",
    "from tf_agents.environments import utils\n",
    "from tf_agents.environments import wrappers\n",
    "from tf_agents.environments import tf_py_environment\n",
    "from tf_agents.networks import sequential\n",
    "from tf_agents.agents.dqn import dqn_agent\n",
    "from tf_agents.specs import tensor_spec\n",
    "from tf_agents.utils import common\n",
    "\n",
    "from tf_agents.agents.dqn import dqn_agent\n",
    "from tf_agents.drivers import py_driver\n",
    "from tf_agents.environments import suite_gym\n",
    "from tf_agents.environments import tf_py_environment\n",
    "from tf_agents.eval import metric_utils\n",
    "from tf_agents.metrics import tf_metrics\n",
    "from tf_agents.networks import sequential\n",
    "from tf_agents.policies import py_tf_eager_policy\n",
    "from tf_agents.policies import random_tf_policy\n",
    "from tf_agents.replay_buffers import reverb_replay_buffer\n",
    "from tf_agents.replay_buffers import reverb_utils\n",
    "from tf_agents.trajectories import trajectory\n",
    "from tf_agents.specs import tensor_spec\n",
    "from tf_agents.utils import common\n",
    "\n",
    "import tensorflow as tf\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_iterations = 20000 # @param {type:\"integer\"}\n",
    "\n",
    "initial_collect_steps = 100  # @param {type:\"integer\"}\n",
    "collect_steps_per_iteration =   1# @param {type:\"integer\"}\n",
    "replay_buffer_max_length = 100000  # @param {type:\"integer\"}\n",
    "\n",
    "batch_size = 64  # @param {type:\"integer\"}\n",
    "learning_rate = 1e-3  # @param {type:\"number\"}\n",
    "log_interval = 200  # @param {type:\"integer\"}\n",
    "\n",
    "num_eval_episodes = 10  # @param {type:\"integer\"}\n",
    "eval_interval = 1000  # @param {type:\"integer\"}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_py_env = Env2048()\n",
    "eval_py_env = Env2048()\n",
    "train_env = tf_py_environment.TFPyEnvironment(train_py_env)\n",
    "eval_env = tf_py_environment.TFPyEnvironment(eval_py_env)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "fc_layer_params = (100, 50)\n",
    "action_tensor_spec = tensor_spec.from_spec(train_env.action_spec())\n",
    "num_actions = action_tensor_spec.maximum - action_tensor_spec.minimum + 1\n",
    "\n",
    "# Define a helper function to create Dense layers configured with the right\n",
    "# activation and kernel initializer.\n",
    "def dense_layer(num_units):\n",
    "  return tf.keras.layers.Dense(\n",
    "      num_units,\n",
    "      activation=tf.keras.activations.relu,\n",
    "      kernel_initializer=tf.keras.initializers.VarianceScaling(\n",
    "          scale=2.0, mode='fan_in', distribution='truncated_normal'))\n",
    "\n",
    "# QNetwork consists of a sequence of Dense layers followed by a dense layer\n",
    "# with `num_actions` units to generate one q_value per available action as\n",
    "# its output.\n",
    "dense_layers = [dense_layer(num_units) for num_units in fc_layer_params]\n",
    "q_values_layer = tf.keras.layers.Dense(\n",
    "    num_actions,\n",
    "    activation=None,\n",
    "    kernel_initializer=tf.keras.initializers.RandomUniform(\n",
    "        minval=-0.03, maxval=0.03),\n",
    "    bias_initializer=tf.keras.initializers.Constant(-0.2))\n",
    "q_net = sequential.Sequential(dense_layers + [q_values_layer])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = tf.keras.optimizers.Adam(learning_rate=learning_rate)\n",
    "\n",
    "train_step_counter = tf.Variable(0)\n",
    "\n",
    "agent = dqn_agent.DqnAgent(\n",
    "    time_step_spec = train_env.time_step_spec(),\n",
    "    action_spec = train_env.action_spec(),\n",
    "    q_network=q_net,\n",
    "    optimizer=optimizer,\n",
    "    td_errors_loss_fn=common.element_wise_squared_loss,\n",
    "    train_step_counter=train_step_counter)\n",
    "\n",
    "agent.initialize()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_policy = agent.policy\n",
    "collect_policy = agent.collect_policy\n",
    "\n",
    "random_policy = random_tf_policy.RandomTFPolicy(train_env.time_step_spec(),\n",
    "                                                train_env.action_spec())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# example_environment = tf_py_environment.TFPyEnvironment(\n",
    "#     suite_gym.load('CartPole-v0'))\n",
    "\n",
    "# time_step = example_environment.reset()\n",
    "\n",
    "# random_policy.action(time_step)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "170.8"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def compute_avg_return(environment, policy, num_episodes=10):\n",
    "\n",
    "  total_return = 0.0\n",
    "  for i in range(num_episodes):\n",
    "    time_step = environment.reset()\n",
    "    episode_return = 0.0\n",
    "\n",
    "    while not time_step.is_last():\n",
    "        action_step = policy.action(time_step)\n",
    "        time_step = environment.step(action_step.action)\n",
    "        episode_return += time_step.reward\n",
    "\n",
    "    total_return += episode_return\n",
    "\n",
    "  avg_return = total_return / num_episodes\n",
    "  return avg_return.numpy()[0]\n",
    "\n",
    "compute_avg_return(eval_env, random_policy, num_eval_episodes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Replay Buffer\n",
    "\n",
    "In order to keep track of the data collected from the environment, we will use [Reverb](https://deepmind.com/research/open-source/Reverb), an efficient, extensible, and easy-to-use replay system by Deepmind. It stores experience data when we collect trajectories and is consumed during training.\n",
    "\n",
    "This replay buffer is constructed using specs describing the tensors that are to be stored, which can be obtained from the agent using agent.collect_data_spec."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "table_name = 'uniform_table'\n",
    "replay_buffer_signature = tensor_spec.from_spec(\n",
    "      agent.collect_data_spec)\n",
    "replay_buffer_signature = tensor_spec.add_outer_dim(\n",
    "    replay_buffer_signature)\n",
    "\n",
    "table = reverb.Table(\n",
    "    table_name,\n",
    "    max_size=replay_buffer_max_length,\n",
    "    sampler=reverb.selectors.Uniform(),\n",
    "    remover=reverb.selectors.Fifo(),\n",
    "    rate_limiter=reverb.rate_limiters.MinSize(1),\n",
    "    signature=replay_buffer_signature)\n",
    "\n",
    "reverb_server = reverb.Server([table])\n",
    "\n",
    "replay_buffer = reverb_replay_buffer.ReverbReplayBuffer(\n",
    "    agent.collect_data_spec,\n",
    "    table_name=table_name,\n",
    "    sequence_length=2,\n",
    "    local_server=reverb_server)\n",
    "\n",
    "rb_observer = reverb_utils.ReverbAddTrajectoryObserver(\n",
    "  replay_buffer.py_client,\n",
    "  table_name,\n",
    "  sequence_length=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Collection\n",
    "\n",
    "Now execute the random policy in the environment for a few steps, recording the data in the replay buffer.\n",
    "\n",
    "Here we are using 'PyDriver' to run the experience collecting loop. You can learn more about TF Agents driver in our [drivers tutorial](https://www.tensorflow.org/agents/tutorials/4_drivers_tutorial)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'env' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-22-54355604438d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m#@test {\"skip\": true}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m py_driver.PyDriver(\n\u001b[0;32m----> 3\u001b[0;31m     \u001b[0menv\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m     py_tf_eager_policy.PyTFEagerPolicy(\n\u001b[1;32m      5\u001b[0m       random_policy, use_tf_function=True),\n",
      "\u001b[0;31mNameError\u001b[0m: name 'env' is not defined"
     ]
    }
   ],
   "source": [
    "#@test {\"skip\": true}\n",
    "py_driver.PyDriver(\n",
    "    env,\n",
    "    py_tf_eager_policy.PyTFEagerPolicy(\n",
    "      random_policy, use_tf_function=True),\n",
    "    [rb_observer],\n",
    "    max_steps=initial_collect_steps).run(train_py_env.reset())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The agent needs access to the replay buffer. This is provided by creating an iterable `tf.data.Dataset` pipeline which will feed data to the agent.\n",
    "\n",
    "Each row of the replay buffer only stores a single observation step. But since the DQN Agent needs both the current and next observation to compute the loss, the dataset pipeline will sample two adjacent rows for each item in the batch (`num_steps=2`).\n",
    "\n",
    "This dataset is also optimized by running parallel calls and prefetching data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<PrefetchDataset element_spec=(Trajectory(\n",
       "{'action': TensorSpec(shape=(64, 2), dtype=tf.int32, name=None),\n",
       " 'discount': TensorSpec(shape=(64, 2), dtype=tf.float32, name=None),\n",
       " 'next_step_type': TensorSpec(shape=(64, 2), dtype=tf.int32, name=None),\n",
       " 'observation': TensorSpec(shape=(64, 2, 16), dtype=tf.int32, name=None),\n",
       " 'policy_info': (),\n",
       " 'reward': TensorSpec(shape=(64, 2), dtype=tf.float32, name=None),\n",
       " 'step_type': TensorSpec(shape=(64, 2), dtype=tf.int32, name=None)}), SampleInfo(key=TensorSpec(shape=(64, 2), dtype=tf.uint64, name=None), probability=TensorSpec(shape=(64, 2), dtype=tf.float64, name=None), table_size=TensorSpec(shape=(64, 2), dtype=tf.int64, name=None), priority=TensorSpec(shape=(64, 2), dtype=tf.float64, name=None), times_sampled=TensorSpec(shape=(64, 2), dtype=tf.int32, name=None)))>"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Dataset generates trajectories with shape [Bx2x...]\n",
    "dataset = replay_buffer.as_dataset(\n",
    "    num_parallel_calls=3,\n",
    "    sample_batch_size=batch_size,\n",
    "    num_steps=2).prefetch(3)\n",
    "\n",
    "iterator = iter(dataset)\n",
    "\n",
    "dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training the agent\n",
    "\n",
    "Two things must happen during the training loop:\n",
    "\n",
    "-   collect data from the environment\n",
    "-   use that data to train the agent's neural network(s)\n",
    "\n",
    "This example also periodicially evaluates the policy and prints the current score.\n",
    "\n",
    "The following will take ~5 minutes to run."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n"
     ]
    },
    {
     "ename": "InvalidArgumentError",
     "evalue": "Received incompatible tensor at flattened index 3 from table 'uniform_table'.  Specification has (dtype, shape): (int32, [?]).  Tensor has (dtype, shape): (int32, [2,1]).\nTable signature: 0: Tensor<name: 'step_type/step_type', dtype: int32, shape: [?]>, 1: Tensor<name: 'observation/observation', dtype: int32, shape: [?,16]>, 2: Tensor<name: 'action/action', dtype: int32, shape: [?]>, 3: Tensor<name: 'next_step_type/step_type', dtype: int32, shape: [?]>, 4: Tensor<name: 'reward/reward', dtype: float, shape: [?]>, 5: Tensor<name: 'discount/discount', dtype: float, shape: [?]> [Op:IteratorGetNext]",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mInvalidArgumentError\u001b[0m                      Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-41-592743b92826>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     35\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     36\u001b[0m     \u001b[0;31m# Sample a batch of data from the buffer and update the agent's network.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 37\u001b[0;31m     \u001b[0mexperience\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0munused_info\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     38\u001b[0m     \u001b[0mtrain_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0magent\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mexperience\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     39\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/tensorflow/python/data/ops/iterator_ops.py\u001b[0m in \u001b[0;36m__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    764\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m__next__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    765\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 766\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_next_internal\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    767\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOutOfRangeError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    768\u001b[0m       \u001b[0;32mraise\u001b[0m \u001b[0mStopIteration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/tensorflow/python/data/ops/iterator_ops.py\u001b[0m in \u001b[0;36m_next_internal\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    747\u001b[0m     \u001b[0;31m# to communicate that there is no more data to iterate over.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    748\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mcontext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexecution_mode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcontext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSYNC\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 749\u001b[0;31m       ret = gen_dataset_ops.iterator_get_next(\n\u001b[0m\u001b[1;32m    750\u001b[0m           \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_iterator_resource\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    751\u001b[0m           \u001b[0moutput_types\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_flat_output_types\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/tensorflow/python/ops/gen_dataset_ops.py\u001b[0m in \u001b[0;36miterator_get_next\u001b[0;34m(iterator, output_types, output_shapes, name)\u001b[0m\n\u001b[1;32m   3015\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0m_result\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3016\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0m_core\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3017\u001b[0;31m       \u001b[0m_ops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mraise_from_not_ok_status\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3018\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0m_core\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_FallbackException\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3019\u001b[0m       \u001b[0;32mpass\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/tensorflow/python/framework/ops.py\u001b[0m in \u001b[0;36mraise_from_not_ok_status\u001b[0;34m(e, name)\u001b[0m\n\u001b[1;32m   7162\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mraise_from_not_ok_status\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   7163\u001b[0m   \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmessage\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m\" name: \"\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;34m\"\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 7164\u001b[0;31m   \u001b[0;32mraise\u001b[0m \u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_status_to_exception\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m  \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   7165\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   7166\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mInvalidArgumentError\u001b[0m: Received incompatible tensor at flattened index 3 from table 'uniform_table'.  Specification has (dtype, shape): (int32, [?]).  Tensor has (dtype, shape): (int32, [2,1]).\nTable signature: 0: Tensor<name: 'step_type/step_type', dtype: int32, shape: [?]>, 1: Tensor<name: 'observation/observation', dtype: int32, shape: [?,16]>, 2: Tensor<name: 'action/action', dtype: int32, shape: [?]>, 3: Tensor<name: 'next_step_type/step_type', dtype: int32, shape: [?]>, 4: Tensor<name: 'reward/reward', dtype: float, shape: [?]>, 5: Tensor<name: 'discount/discount', dtype: float, shape: [?]> [Op:IteratorGetNext]"
     ]
    }
   ],
   "source": [
    "#@test {\"skip\": true}\n",
    "try:\n",
    "  %%time\n",
    "except:\n",
    "  pass\n",
    "\n",
    "# (Optional) Optimize by wrapping some of the code in a graph using TF function.\n",
    "agent.train = common.function(agent.train)\n",
    "\n",
    "# Reset the train step.\n",
    "agent.train_step_counter.assign(0)\n",
    "\n",
    "\n",
    "# Evaluate the agent's policy once before training.\n",
    "avg_return = compute_avg_return(eval_env, agent.policy, num_eval_episodes)\n",
    "returns = [avg_return]\n",
    "\n",
    "# Reset the environment.\n",
    "time_step = train_py_env.reset()\n",
    "\n",
    "# Create a driver to collect experience.\n",
    "collect_driver = py_driver.PyDriver(\n",
    "    #TODO: Ce env bagi aici?\n",
    "    train_env,\n",
    "    py_tf_eager_policy.PyTFEagerPolicy(\n",
    "      agent.collect_policy, use_tf_function=True),\n",
    "    [rb_observer],\n",
    "    max_steps=collect_steps_per_iteration)\n",
    "\n",
    "for i in range(num_iterations):\n",
    "    \n",
    "    print(i)\n",
    "    # Collect a few steps and save to the replay buffer.\n",
    "    time_step, _ = collect_driver.run(time_step)\n",
    "\n",
    "    # Sample a batch of data from the buffer and update the agent's network.\n",
    "    experience, unused_info = next(iterator)\n",
    "    train_loss = agent.train(experience).loss\n",
    "\n",
    "    step = agent.train_step_counter.numpy()\n",
    "\n",
    "    if step % log_interval == 0:\n",
    "        print('step = {0}: loss = {1}'.format(step, train_loss))\n",
    "\n",
    "    if step % eval_interval == 0:\n",
    "        avg_return = compute_avg_return(eval_env, agent.policy, num_eval_episodes)\n",
    "        print('step = {0}: Average Return = {1}'.format(step, avg_return))\n",
    "        returns.append(avg_return)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## -----------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "100\n",
      "200\n",
      "300\n",
      "400\n",
      "500\n",
      "600\n",
      "700\n",
      "800\n",
      "900\n",
      "1000\n",
      "1100\n",
      "1200\n",
      "1300\n",
      "1400\n",
      "1500\n",
      "1600\n",
      "1700\n",
      "1800\n",
      "1900\n",
      "2000\n",
      "2100\n",
      "2200\n",
      "2300\n",
      "2400\n",
      "2500\n",
      "2600\n",
      "2700\n",
      "2800\n",
      "2900\n"
     ]
    }
   ],
   "source": [
    "env = tf_py_environment.TFPyEnvironment(Env2048())\n",
    "# env = Env2048()\n",
    "\n",
    "rewards = []\n",
    "max_rewards = []\n",
    "steps = []\n",
    "total_rewards = []\n",
    "env.reset()\n",
    "\n",
    "for i in range(3000):\n",
    "    if not i % 100:\n",
    "        print(i)\n",
    "    time_step = env.step(np.array(np.random.randint(4), dtype=np.int32))\n",
    "    while not time_step.is_last():\n",
    "        time_step = env.step(np.array(np.random.randint(4), dtype=np.int32))\n",
    "    if time_step.is_last():\n",
    "        pyenv = env.pyenv.envs[0]\n",
    "#         pyenv = env\n",
    "        rewards += pyenv.get_state().reward_history\n",
    "        steps += [pyenv.get_state().step_count]\n",
    "        total_rewards += [pyenv.get_state().total_reward]\n",
    "        max_rewards += [pyenv.get_state().max_reward]\n",
    "        env.reset()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([ 12.,  96., 132.,  11., 495., 349., 232.,  41.,  35.,   2., 364.,\n",
       "        501., 323.,  84.,  82.,  18.,   4.,   6.,   5.,   1.,   0.,   0.,\n",
       "          0.,  56.,  70.,  43.,  17.,  13.,   2.,   2.,   3.,   0.,   0.,\n",
       "          0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
       "          0.,   0.,   0.,   0.,   0.,   1.]),\n",
       " array([ 20.  ,  29.92,  39.84,  49.76,  59.68,  69.6 ,  79.52,  89.44,\n",
       "         99.36, 109.28, 119.2 , 129.12, 139.04, 148.96, 158.88, 168.8 ,\n",
       "        178.72, 188.64, 198.56, 208.48, 218.4 , 228.32, 238.24, 248.16,\n",
       "        258.08, 268.  , 277.92, 287.84, 297.76, 307.68, 317.6 , 327.52,\n",
       "        337.44, 347.36, 357.28, 367.2 , 377.12, 387.04, 396.96, 406.88,\n",
       "        416.8 , 426.72, 436.64, 446.56, 456.48, 466.4 , 476.32, 486.24,\n",
       "        496.16, 506.08, 516.  ]),\n",
       " <BarContainer object of 50 artists>)"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "# plt.hist(rewards, bins=50)\n",
    "# plt.hist(steps, bins=50)\n",
    "# plt.hist(total_rewards, bins=50)\n",
    "plt.hist(max_rewards, bins=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python3_8",
   "language": "python",
   "name": "python3_8"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
